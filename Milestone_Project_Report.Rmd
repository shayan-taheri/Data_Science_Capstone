---
title: "Milestone Project: Exploratory Data Analysis and Statistical Data Modeling"
author: "Shayan (Sean) Taheri"
date: "June 22, 2019"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This report belongs to the milestone project of the Data Science Capstone in the Data Science Specialization from Coursera. The project objective is creation of a predictive text model accoring to which it predicts the next word in a sequence with preceding word/words. The model is evaluated on three different datasets of English-based twitter, blog, and news that are all text-based [1]. In development of this model, we use Natural Language Processing (NLP) techniques. These techniques can perform text-based analysis and build a predictive model. Next, a Shiny application is built for both user-interface and server versions using the constructed model [2]. This application has a similar duty to what exists in the text-messaging applications.

The main goal of the project can be defined as: (a) ability of analyzing the datasets; (b) writing a report containing statistical summary of the datasets; (c) providing a report based on the findings from the datasets; (d) showing the prediction algorithm ability; (e) evaluating the prediction system based on received feedbacks from these datasets. This project helps us to gain a number of skills of a data scientist including: (1) providing basic summaries of the three files namely word counts, line counts, and a basic data tables; (2) making basic plots for demonstrating the features of the data, such as histograms. (3) creation of a report understandable for non-data scientist manager or investor.

# Analysis of Datasets

## Task 1: Loading Libraries and Datasets

In the first step, the datasets are loaded into the system using "read_lines()" function. 

```{r, warning=FALSE, cache=TRUE, include=FALSE, echo=FALSE, message = FALSE}
library(readr)
library(ngram)
library(kableExtra)
library(SnowballC)
library(tm)
library(tmap)
library(corpora)
library(dplyr)
library(data.table)
library(ggplot2)
library(ggthemes)
library(wordcloud)
library(stringr)
library(tidytext)
library(frequency)
library(tidyr)
library(slam)
library(textstem)
library(RWeka)

# Loading Dataset Files:

blogs_file <- "C:/Users/shaya/Desktop/Data_Science_Capstone/SwiftKey_Dataset/en_US/en_US.blogs.txt"
news_file <- "C:/Users/shaya/Desktop/Data_Science_Capstone/SwiftKey_Dataset/en_US/en_US.news.txt"
twitter_file <- "C:/Users/shaya/Desktop/Data_Science_Capstone/SwiftKey_Dataset/en_US/en_US.twitter.txt"

# Read the data files

blogData   <- read_lines(blogs_file)
newsData    <- read_lines(news_file)
twitterData <- read_lines(twitter_file)
```

## Task 2: Exploratory Data Analysis

### Summary Statistics

Regarding one of the requirements of this project, the "Word Count" and the "Line Count" are reported as the summary statistics of these datasets.

```{r, warning=FALSE, cache=TRUE, message = FALSE, echo=FALSE}
docCounts = data.frame(Media=c("Twitter"
                               ,"Blog"
                               ,"News")
                       ,Word_Count=c(sum(stringr::str_count(twitterData, "\\S+"))
                                     ,sum(stringr::str_count(blogData, "\\S+"))
                                     ,sum(stringr::str_count(newsData, "\\S+"))
                       )
)
docCounts$Line_Count = c(length(twitterData)
                         ,length(blogData)
                         ,length(newsData))
docCounts
```

As it can be observed from these statistics, the Tweets show a greater number than News, while Blogs display a smaller number than the other two ones. we have similar word counts among all three datasets.

### Sampling Data and Unigrams Analysis
In the first step of NLP analysis, the unigrams are extracted by finding the most common words without any stemming, stop-word removal, and lemmatization. We sample down data since the ordinary running machine cannot handle all of those documents. Also, sampling the datasets makes it easier for us to work with them. In order to see the unigrams analysis, the histograms of 30 common words are displayed.

```{r, warning=FALSE, cache=TRUE, message = FALSE, include=FALSE, echo=FALSE}
set.seed(5-6-2019)
twitterData <- twitterData[sample(1:length(twitterData),20000)]
blogData <- blogData[sample(1:length(blogData),20000)]
newsData <- newsData[sample(1:length(newsData),20000)]
corpusTwitter <- VCorpus(VectorSource(twitterData))
corpusBlog <- VCorpus(VectorSource(blogData))
corpusNews <- VCorpus(VectorSource(newsData))
DTMtwitter <- DocumentTermMatrix(corpusTwitter
                                 , control = list(tokenize="words"
                                                  ,removePunctuation = FALSE
                                                  ,stopwords = FALSE
                                                  ,stemming = FALSE
                                                  ,tolower = FALSE
                                 )
)
DTMblog <- DocumentTermMatrix(corpusBlog
                              , control = list(tokenize="words"
                                               ,removePunctuation = FALSE
                                               ,stopwords = FALSE
                                               ,stemming = FALSE
                                               ,tolower = FALSE
                              )
)
DTMnews <- DocumentTermMatrix(corpusNews
                              , control = list(tokenize="words"
                                               ,removePunctuation = FALSE
                                               ,stopwords = FALSE
                                               ,stemming = FALSE
                                               ,tolower = FALSE
                              )
)
twitterCounts <- data.frame(col_sums(DTMtwitter))
twitterCounts$term <- row.names(twitterCounts)
blogCounts <- data.frame(col_sums(DTMblog))
blogCounts$term <- row.names(blogCounts)
newsCounts <- data.frame(col_sums(DTMnews))
newsCounts$term <- row.names(newsCounts)
twitterCounts <- twitterCounts[order(twitterCounts$col_sums.DTMtwitter., decreasing = TRUE),]
blogCounts <- blogCounts[order(blogCounts$col_sums.DTMblog., decreasing = TRUE),]
newsCounts <- newsCounts[order(newsCounts$col_sums.DTMnews., decreasing = TRUE),]
twitterCounts <- twitterCounts[1:30,]
twitterCounts$term <- factor(twitterCounts$term, levels = twitterCounts$term)
blogCounts <- blogCounts[1:30,]
blogCounts$term <- factor(blogCounts$term, levels = blogCounts$term)
newsCounts <- newsCounts[1:30,]
newsCounts$term <- factor(newsCounts$term, levels = newsCounts$term)
```

```{r, warning=FALSE, cache=TRUE, message = FALSE, echo=FALSE}
print(ggplot(data = twitterCounts, aes(x=term, y=col_sums.DTMtwitter.)) + geom_bar(fill="blue", stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("Unigram (Word)") + ylab("Frequency in Sample") + ggtitle("Twitter Dataset Unigram Frequency"))
print(ggplot(data = blogCounts, aes(x=term, y=col_sums.DTMblog.)) + geom_bar(fill="red", stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("Unigram (Word)") + ylab("Frequency in Sample") + ggtitle("Blog Dataset Unigram Frequency"))
print(ggplot(data = newsCounts, aes(x=term, y=col_sums.DTMnews.)) + geom_bar(fill="green", stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("Unigram (Word)") + ylab("Frequency in Sample") + ggtitle("News Dataset Unigram Frequency"))
```

As it can be observed from the plots, the achieved results look reasonable since the words of "the", "and", "for", "that", and "with" appear more frequently than the other words. Also, the word of "you"is more present in  "tweets" dataset than in "blogs" dataset. Not only that, it is less present in "news" dataset than the other two datasets. Due to these differences, a single model cannot performs the prediction function with high accuracy and distinguished models are needed for different datasets. 

Regarding this statement, the bigrams and the trigrams are extracted after purging the main body of datasets. In order to purge the datasets, all the punctuations, numbers, lowercase letters, and extra whitespaces are removed from texts and they are lemmatized. Although the whitespaces are the most important ones since prediction of the word that comes after other words, numbers, and punctuations is the main priority. After these data pre-processing step, the bigram/trigram analysis will be executed.

```{r, warning=FALSE, cache=TRUE, message = FALSE, include=FALSE, echo=FALSE}
corpusTwitter <- tm_map(corpusTwitter, removePunctuation)
corpusTwitter <- tm_map(corpusTwitter, removeNumbers)
corpusTwitter <- tm_map(corpusTwitter, content_transformer(tolower))
corpusTwitter <- tm_map(corpusTwitter, removeWords, stopwords("en"))
corpusTwitter <- tm_map(corpusTwitter, stripWhitespace)
corpusTwitter <- tm_map(corpusTwitter, content_transformer(lemmatize_words))

corpusBlog <- tm_map(corpusBlog, removePunctuation)
corpusBlog <- tm_map(corpusBlog, removeNumbers)
corpusBlog <- tm_map(corpusBlog, content_transformer(tolower))
corpusBlog <- tm_map(corpusBlog, removeWords, stopwords("en"))
corpusBlog <- tm_map(corpusBlog, stripWhitespace)
corpusBlog <- tm_map(corpusBlog, content_transformer(lemmatize_words))

corpusNews <- tm_map(corpusNews, removePunctuation)
corpusNews <- tm_map(corpusNews, removeNumbers)
corpusNews <- tm_map(corpusNews, content_transformer(tolower))
corpusNews <- tm_map(corpusNews, removeWords, stopwords("en"))
corpusNews <- tm_map(corpusNews, stripWhitespace)
corpusNews <- tm_map(corpusNews, content_transformer(lemmatize_words))
```

### Bigrams and Trigrams Analysis

In order to extract the bigrams and the unigrams of these datasets, two functions of "BigramTokenizer()"" and "TrigramTokenizer()" are employed.

```{r, warning=FALSE, cache=TRUE, message = FALSE, echo=FALSE}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2, delimiters = " "))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3, delimiters = " "))
DTMtwitter <- DocumentTermMatrix(corpusTwitter, control=list(tokenize=BigramTokenizer
                                                             ,removePunctuation = FALSE
                                                             ,stopwords = FALSE
                                                             ,stemming = FALSE
                                                             ,tolower = FALSE
)
)
DTMblog <- DocumentTermMatrix(corpusBlog, control=list(tokenize=BigramTokenizer
                                                       ,removePunctuation = FALSE
                                                       ,stopwords = FALSE
                                                       ,stemming = FALSE
                                                       ,tolower = FALSE
)
)
DTMnews <- DocumentTermMatrix(corpusNews, control=list(tokenize=BigramTokenizer
                                                       ,removePunctuation = FALSE
                                                       ,stopwords = FALSE
                                                       ,stemming = FALSE
                                                       ,tolower = FALSE
)
)
twitterCounts <- data.frame(col_sums(DTMtwitter))
twitterCounts$term <- row.names(twitterCounts)
blogCounts <- data.frame(col_sums(DTMblog))
blogCounts$term <- row.names(blogCounts)
newsCounts <- data.frame(col_sums(DTMnews))
newsCounts$term <- row.names(newsCounts)
twitterCounts <- twitterCounts[order(twitterCounts$col_sums.DTMtwitter., decreasing = TRUE),]
blogCounts <- blogCounts[order(blogCounts$col_sums.DTMblog., decreasing = TRUE),]
newsCounts <- newsCounts[order(newsCounts$col_sums.DTMnews., decreasing = TRUE),]

twitterCounts <- twitterCounts[1:30,]
twitterCounts$term <- factor(twitterCounts$term, levels = twitterCounts$term)
blogCounts <- blogCounts[1:30,]
blogCounts$term <- factor(blogCounts$term, levels = blogCounts$term)
newsCounts <- newsCounts[1:30,]
newsCounts$term <- factor(newsCounts$term, levels = newsCounts$term)
print(ggplot(data = twitterCounts, aes(x=term, y=col_sums.DTMtwitter.)) + geom_bar(fill="blue", stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1))+ xlab("Bigram") + ylab("Frequency in Sample") + ggtitle("Twitter Dataset Bigram Frequency"))
print(ggplot(data = blogCounts, aes(x=term, y=col_sums.DTMblog.)) + geom_bar(fill="red", stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("Bigram") + ylab("Frequency in Sample") + ggtitle("Blog Dataset Bigram Frequency"))
print(ggplot(data = newsCounts, aes(x=term, y=col_sums.DTMnews.)) + geom_bar(fill="green", stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("Bigram") + ylab("Frequency in Sample") + ggtitle("News Dataset Bigram Frequency"))
DTMtwitter <- DocumentTermMatrix(corpusTwitter, control=list(tokenize=TrigramTokenizer
                                                             ,removePunctuation = FALSE
                                                             ,stopwords = FALSE
                                                             ,stemming = FALSE
                                                             ,tolower = FALSE
)
)
DTMblog <- DocumentTermMatrix(corpusBlog, control=list(tokenize=TrigramTokenizer
                                                       ,removePunctuation = FALSE
                                                       ,stopwords = FALSE
                                                       ,stemming = FALSE
                                                       ,tolower = FALSE
)
)
DTMnews <- DocumentTermMatrix(corpusNews, control=list(tokenize=TrigramTokenizer
                                                       ,removePunctuation = FALSE
                                                       ,stopwords = FALSE
                                                       ,stemming = FALSE
                                                       ,tolower = FALSE
)
)
twitterCounts <- data.frame(col_sums(DTMtwitter))
twitterCounts$term <- row.names(twitterCounts)
blogCounts <- data.frame(col_sums(DTMblog))
blogCounts$term <- row.names(blogCounts)
newsCounts <- data.frame(col_sums(DTMnews))
newsCounts$term <- row.names(newsCounts)
twitterCounts <- twitterCounts[order(twitterCounts$col_sums.DTMtwitter., decreasing = TRUE),]
blogCounts <- blogCounts[order(blogCounts$col_sums.DTMblog., decreasing = TRUE),]
newsCounts <- newsCounts[order(newsCounts$col_sums.DTMnews., decreasing = TRUE),]
twitterCounts <- twitterCounts[1:30,]
twitterCounts$term <- factor(twitterCounts$term, levels = twitterCounts$term)
blogCounts <- blogCounts[1:30,]
blogCounts$term <- factor(blogCounts$term, levels = blogCounts$term)
newsCounts <- newsCounts[1:30,]
newsCounts$term <- factor(newsCounts$term, levels = newsCounts$term)
print(ggplot(data = twitterCounts, aes(x=term, y=col_sums.DTMtwitter.)) + geom_bar(fill="blue", stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("Trigram") + ylab("Frequency in Sample") + ggtitle("Twitter Dataset Trigram Frequency"))
print(ggplot(data = blogCounts, aes(x=term, y=col_sums.DTMblog.)) + geom_bar(fill="red", stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("Trigram") + ylab("Frequency in Sample") + ggtitle("Blog Dataset Trigram Frequency"))
print(ggplot(data = newsCounts, aes(x=term, y=col_sums.DTMnews.)) + geom_bar(fill="green", stat="identity") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + xlab("Trigram") + ylab("Frequency in Sample") + ggtitle("News Dataset Trigram Frequency"))
```

Due to the sufficient differences among these datasets, it is required to build distinguished predictions for them. Although the similarities among them cannot be neglected. After completion of the prediction algorithm with the function of predicting the next word in a sequence of text, it is necessary to deploy the model into a Shiny application for both user-interface and server version. All the source codes related to this project can be found in [3].

## Conclusion and Future Work
This report provides an exploratory analysis on three datasets. Through this analysis, the unigrams, the bigrams, and the trigrams of three datasets of "blogs", "news", and "twitter" are extracted. These models can be employed step-by-step into the final product. That means using n-gram model with a frequency lookup similar to the presented ones in this report. Through this employment, the highest gram model is used to predict the next word. If it is not working, then a lower level of n-gram is used. The future work of this project is deploying the predictive algorithm into a Shiny application with the user-interface and the server versions.

# References
[1] https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

[2] https://www.shinyapps.io

[3] https://github.com/shayan-taheri/Data_Science_Capstone
